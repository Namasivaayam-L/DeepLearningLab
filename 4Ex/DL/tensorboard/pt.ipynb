{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "# writer = SummaryWriter(log_dir='pt_logs')\n",
    "# # Load the dataset and perform exploratory data analysis\n",
    "# from sklearn.datasets import load_digits\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# digits = load_digits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot some examples of the images\n",
    "# for i in range(2):\n",
    "#     plt.imshow(digits.images[i], cmap='gray')\n",
    "#     plt.title(f\"Label: {digits.target[i]}\")\n",
    "#     plt.show()\n",
    "transform = transforms.Compose([transforms.Resize((64,64)),\n",
    "                                 transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the data for training\n",
    "# X = digits.images.reshape(-1, 64)\n",
    "# y = digits.target\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "# test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# datasets\n",
    "data_dir='Pistachio_Image_Dataset'\n",
    "train_data = torchvision.datasets.ImageFolder(data_dir + '/train', transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(data_dir + '/test', transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Loss: 1.6301928013563156\n",
      "[2] Loss: 0.5760794267058372\n",
      "[3] Loss: 0.38090750202536583\n",
      "[4] Loss: 0.2971792906522751\n",
      "[5] Loss: 0.2406189538538456\n",
      "[6] Loss: 0.2152233999222517\n",
      "[7] Loss: 0.19191288948059082\n",
      "[8] Loss: 0.17874231599271298\n",
      "[9] Loss: 0.1597558006644249\n",
      "[10] Loss: 0.15167962647974492\n",
      "[11] Loss: 0.13709327019751072\n",
      "[12] Loss: 0.13188772238790988\n",
      "[13] Loss: 0.12157265488058329\n",
      "[14] Loss: 0.11528278291225433\n",
      "[15] Loss: 0.10952215790748596\n",
      "[16] Loss: 0.10195382293313741\n",
      "[17] Loss: 0.09898468777537346\n",
      "[18] Loss: 0.0949542822316289\n",
      "[19] Loss: 0.08908933699131012\n",
      "[20] Loss: 0.08744103834033012\n",
      "[21] Loss: 0.0842334596440196\n",
      "[22] Loss: 0.07939304411411285\n",
      "[23] Loss: 0.0792497375048697\n",
      "[24] Loss: 0.07597197089344263\n",
      "[25] Loss: 0.07175044566392899\n",
      "[26] Loss: 0.07061436250805855\n",
      "[27] Loss: 0.06721655931323767\n",
      "[28] Loss: 0.06538268979638814\n",
      "[29] Loss: 0.06284166621044278\n",
      "[30] Loss: 0.06157493330538273\n",
      "[31] Loss: 0.05784272933378816\n",
      "[32] Loss: 0.056562759727239606\n",
      "[33] Loss: 0.05444405470043421\n",
      "[34] Loss: 0.05287857726216316\n",
      "[35] Loss: 0.05275964727625251\n",
      "[36] Loss: 0.04999936958774924\n",
      "[37] Loss: 0.04951706742867827\n",
      "[38] Loss: 0.047692449577152726\n",
      "[39] Loss: 0.04722722265869379\n",
      "[40] Loss: 0.04613832850009203\n",
      "[41] Loss: 0.045022500492632386\n",
      "[42] Loss: 0.04399530463851988\n",
      "[43] Loss: 0.04236898608505726\n",
      "[44] Loss: 0.04239839790388942\n",
      "[45] Loss: 0.04122450607828796\n",
      "[46] Loss: 0.039187158737331626\n",
      "[47] Loss: 0.038379755709320305\n",
      "[48] Loss: 0.038759149797260764\n",
      "[49] Loss: 0.03628002442419529\n",
      "[50] Loss: 0.03569215340539813\n",
      "[51] Loss: 0.03588347220793366\n",
      "[52] Loss: 0.03459775121882558\n",
      "[53] Loss: 0.033472059108316896\n",
      "[54] Loss: 0.03231466487050057\n",
      "[55] Loss: 0.03318890938535333\n",
      "[56] Loss: 0.031874613184481856\n",
      "[57] Loss: 0.031155737163499\n",
      "[58] Loss: 0.030575250554829837\n",
      "[59] Loss: 0.029674292355775834\n",
      "[60] Loss: 0.029085510270670058\n",
      "[61] Loss: 0.029691967507824303\n",
      "[62] Loss: 0.028255915502086282\n",
      "[63] Loss: 0.028123900899663566\n",
      "[64] Loss: 0.027255345601588488\n",
      "[65] Loss: 0.026626227144151925\n",
      "[66] Loss: 0.026440770737826823\n",
      "[67] Loss: 0.025501248287037016\n",
      "[68] Loss: 0.02523072101175785\n",
      "[69] Loss: 0.025051389681175353\n",
      "[70] Loss: 0.024525045044720174\n",
      "[71] Loss: 0.02445951020345092\n",
      "[72] Loss: 0.023958846041932703\n",
      "[73] Loss: 0.02358678667806089\n",
      "[74] Loss: 0.023274109745398164\n",
      "[75] Loss: 0.02271910267882049\n",
      "[76] Loss: 0.02236530208028853\n",
      "[77] Loss: 0.021861155284568667\n",
      "[78] Loss: 0.02153233946301043\n",
      "[79] Loss: 0.020981641905382275\n",
      "[80] Loss: 0.020665168482810258\n",
      "[81] Loss: 0.020853228913620115\n",
      "[82] Loss: 0.02045739428140223\n",
      "[83] Loss: 0.019952605199068784\n",
      "[84] Loss: 0.019821073114871978\n",
      "[85] Loss: 0.019114676117897033\n",
      "[86] Loss: 0.019127117283642293\n",
      "[87] Loss: 0.019070830708369613\n",
      "[88] Loss: 0.018664450198411942\n",
      "[89] Loss: 0.018269937485456467\n",
      "[90] Loss: 0.01844566660001874\n",
      "[91] Loss: 0.01766859912313521\n",
      "[92] Loss: 0.0176204742398113\n",
      "[93] Loss: 0.017440593242645262\n",
      "[94] Loss: 0.017337384913116694\n",
      "[95] Loss: 0.017077127937227488\n",
      "[96] Loss: 0.017336402833461762\n",
      "[97] Loss: 0.016506231995299457\n",
      "[98] Loss: 0.016473944229073823\n",
      "[99] Loss: 0.016609999956563114\n",
      "[100] Loss: 0.01566750705242157\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = MLP()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "        # writer.add_scalar('Loss/test', , epoch)\n",
    "        # writer.add_scalar('Accuracy/test', , epoch)\n",
    "    # print(f\"[{epoch + 1}] Loss: {running_loss / (i + 1)}\")\n",
    "    writer.add_graph(model,data[0])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        outputs = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "2023-03-09 15:38:07.823226: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
       "2023-03-09 15:38:08.709863: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
       "2023-03-09 15:38:08.709943: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
       "2023-03-09 15:38:08.709951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
       "2023-03-09 15:38:10.072141: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: UNKNOWN ERROR (100)\n",
       "2023-03-09 15:38:10.072197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Namachu-Asus-ROG): /proc/driver/nvidia/version does not exist\n",
       "\n",
       "NOTE: Using experimental fast data loading logic. To disable, pass\n",
       "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
       "    https://github.com/tensorflow/tensorboard/issues/4784\n",
       "\n",
       "I0309 15:38:11.190606 139960250242624 plugin.py:429] Monitor runs begin\n",
       "Address already in use\n",
       "Port 6006 is in use by another program. Either identify and stop that program, or start the server with a different port."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "log_folder = './pt_logs'\n",
    "%tensorboard --logdir={log_folder}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2 (default, Feb 28 2021, 17:03:44) \n[GCC 10.2.1 20210110]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
