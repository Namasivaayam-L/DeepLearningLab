{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set dataset path\n",
    "# Load list of class names\n",
    "with open(\"../Datasets/UCF101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/classInd.txt\") as f:\n",
    "    class_names = [line.strip().split()[1] for line in f.readlines()]\n",
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def vid_to_np_arr(filename,filepath,resized_cls_dir):\n",
    "# def vid_to_np_arr(filepath):\n",
    "#     # Load the video file\n",
    "#     # resized_file_path = \"../Datasets/UCF101 Resized/\"\n",
    "#     video = cv2.VideoCapture(filepath)\n",
    "#     frames = []\n",
    "#     while video.isOpened():\n",
    "#         ret, frame = video.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#         frame = cv2.resize(frame, (64,64))\n",
    "#         # Save the resized image\n",
    "#         # file_path = resized_cls_dir + '/' + filename\n",
    "#         # cv2.imwrite(file_path, frame)\n",
    "#         frames.append(np.array(frame))\n",
    "#     np_arry = np.array(frames)\n",
    "#     video.release()\n",
    "#     return np_arry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load training data\n",
    "# train_data = []\n",
    "# train_labels = []\n",
    "# resized_ds_dir = \"../Datasets/UCF101_Resized/UCF101/UCF-101/\"\n",
    "# for class_id, class_name in enumerate(class_names):\n",
    "#     class_dir = os.path.join('../Datasets/UCF101/UCF101/UCF-101/',class_name)\n",
    "#     # os.makedirs(resized_ds_dir+class_name)\n",
    "#     # resized_cls_dir = resized_ds_dir + class_name\n",
    "#     for filename in os.listdir(class_dir)[:20]:\n",
    "#         filepath = os.path.join(class_dir, filename)\n",
    "#         video = np.load(filepath)\n",
    "#         # video = vid_to_np_arr(filepath)\n",
    "#         # video = vid_to_np_arr(filename,filepath,resized_cls_dir)\n",
    "#         train_data.append(video)\n",
    "#         train_labels.append(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load testing data\n",
    "# test_data = []\n",
    "# test_labels = []\n",
    "# with open(\"../Datasets/UCF101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/testlist01.txt\") as f:\n",
    "#     for line in f.readlines()[:20]:\n",
    "#         filename = line.strip().split()[0]\n",
    "#         class_name = filename.split(\"/\")[0]\n",
    "#         class_id = class_names.index(class_name)\n",
    "#         filepath = os.path.join(filename)\n",
    "#         video = np.load(filepath)\n",
    "#         # video = vid_to_np_arr(filepath)\n",
    "#         test_labels.append(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_paths={}\n",
    "base_path = '../Datasets/UCF101/UCF-101/'\n",
    "train_files = os.listdir('../Datasets/UCF101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/')\n",
    "# print(train_files)\n",
    "for file in train_files[:-1]:\n",
    "    if 'train' in file:\n",
    "        with open(\"../Datasets/UCF101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/\"+file, \"r\") as f:\n",
    "            # print(file)\n",
    "            subset_paths['train'] = [base_path+line.strip().split()[0] for line in f.readlines()]\n",
    "            # print([line.split(' ') for line in f.readlines()])\n",
    "            # subset_paths['train_labels'] = [line.strip().split()[1] for line in f.readlines()] \n",
    "    if 'testlist01' in file or 'testlist02' in file:\n",
    "        with open(\"../Datasets/UCF101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/\"+file, \"r\") as f:\n",
    "            # print(file)\n",
    "            subset_paths['val'] = [base_path+line.strip() for line in f.readlines()]\n",
    "    if 'testlist03' in file:\n",
    "        with open(\"../Datasets/UCF101/UCF101TrainTestSplits-RecognitionTask/ucfTrainTestlist/\"+file, \"r\") as f:\n",
    "            # print(file)\n",
    "            subset_paths['test'] = [base_path+line.strip() for line in f.readlines()]\n",
    "\n",
    "random.shuffle(subset_paths['train'])\n",
    "# subset_paths['train']=subset_paths['train'][:50]\n",
    "random.shuffle(subset_paths['test'])\n",
    "# subset_paths['test']=subset_paths['test'][:50]\n",
    "random.shuffle(subset_paths['val'])\n",
    "# subset_paths['val']=subset_paths['val'][:50]\n",
    "# print(subset_paths['test'][:4])\n",
    "# # print(subset_paths['train_labels'][:4])\n",
    "# print(subset_paths['val'][:4])\n",
    "# print(subset_paths['train'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "\n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_from_video_file(video_path, n_frames, output_size = (128,128), frame_step = 15):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(n_frames - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameGenerator:\n",
    "  def __init__(self, path, n_frames, training = False,key='train'):\n",
    "    \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "      Args:\n",
    "        path: Video file paths.\n",
    "        n_frames: Number of frames. \n",
    "        training: Boolean to determine if training dataset is being created.\n",
    "    \"\"\"\n",
    "    self.path = path\n",
    "    self.n_frames = n_frames\n",
    "    self.training = training\n",
    "    self.class_names =  class_names\n",
    "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "    self.key = key\n",
    "  def get_files_and_class_names(self):\n",
    "    return subset_paths[self.key], self.class_names\n",
    "\n",
    "  def __call__(self):\n",
    "    video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "    pairs = list(zip(video_paths, classes))\n",
    "\n",
    "    if self.training:\n",
    "      random.shuffle(pairs)\n",
    "\n",
    "    for path, name in pairs:\n",
    "      video_frames = frames_from_video_file(path, self.n_frames) \n",
    "      label = self.class_ids_for_name[name] # Encode labels\n",
    "      yield video_frames, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 05:35:13.991200: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 67 of 1000\n",
      "2023-04-26 05:35:18.804213: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set of frames: (2, 10, 128, 128, 3)\n",
      "Shape of training labels: (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 05:35:28.954762: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 76 of 1000\n",
      "2023-04-26 05:35:32.328389: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set of frames: (2, 10, 128, 128, 3)\n",
      "Shape of validation labels: (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 05:35:42.436313: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 70 of 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test set of frames: (2, 10, 128, 128, 3)\n",
      "Shape of test labels: (2,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 05:35:46.703923: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n"
     ]
    }
   ],
   "source": [
    "# Create the training set\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], 10, training=True,key='train'),\n",
    "                                          output_signature = output_signature)\n",
    "# Create the validation set\n",
    "val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], 10, key='val'),\n",
    "                                        output_signature = output_signature)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], 10, key='test'),\n",
    "                                        output_signature = output_signature)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size = AUTOTUNE)\n",
    "train_ds = train_ds.batch(2)\n",
    "val_ds = val_ds.batch(2)\n",
    "test_ds = test_ds.batch(2)\n",
    "\n",
    "train_frames, train_labels = next(iter(train_ds))\n",
    "print(f'Shape of training set of frames: {train_frames.shape}')\n",
    "print(f'Shape of training labels: {train_labels.shape}')\n",
    "\n",
    "val_frames, val_labels = next(iter(val_ds))\n",
    "print(f'Shape of validation set of frames: {val_frames.shape}')\n",
    "print(f'Shape of validation labels: {val_labels.shape}')\n",
    "\n",
    "test_frames, test_labels = next(iter(test_ds))\n",
    "print(f'Shape of test set of frames: {test_frames.shape}')\n",
    "print(f'Shape of test labels: {test_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Convert data to tensors and normalize pixel values to range [0, 1]\n",
    "# train_data = tf.convert_to_tensor(train_data, dtype=tf.float32) / 255.\n",
    "# train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
    "# test_data = tf.convert_to_tensor(test_data, dtype=tf.float32) / 255.\n",
    "# test_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)\n",
    "\n",
    "# # Create dataset objects\n",
    "# train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).shuffle(len(train_data)).batch(32)\n",
    "# test_ds = tf.data.Dataset.from_tensor_slices((test_data, test_labels)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_4 (Conv3D)           (None, 8, 126, 126, 64)   5248      \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 4, 63, 63, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 2, 61, 61, 61)     105469    \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 1, 30, 30, 61)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 54900)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               28109312  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 101)               51813     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,271,842\n",
      "Trainable params: 28,271,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the CNN model\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv3D(64, (3, 3, 3), activation='relu', input_shape=(10, 128, 128, 3)),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Conv3D(61, (3, 3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    # tf.keras.layers.Conv3D(61, (3, 3, 3), activation='relu'),\n",
    "    # tf.keras.layers.MaxPooling3D((2, 2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "51/51 [==============================] - 5s 60ms/step - loss: 4.3875 - accuracy: 0.0396 - val_loss: 4.6156 - val_accuracy: 0.0198\n",
      "Epoch 2/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 4.2569 - accuracy: 0.0693 - val_loss: 4.7166 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 3.8091 - accuracy: 0.1683 - val_loss: 4.9672 - val_accuracy: 0.0099\n",
      "Epoch 4/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 2.4771 - accuracy: 0.4653 - val_loss: 6.9977 - val_accuracy: 0.0099\n",
      "Epoch 5/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 1.4846 - accuracy: 0.6634 - val_loss: 8.9558 - val_accuracy: 0.0297\n",
      "Epoch 6/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.9460 - accuracy: 0.7723 - val_loss: 8.0387 - val_accuracy: 0.0099\n",
      "Epoch 7/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.6518 - accuracy: 0.8317 - val_loss: 13.2826 - val_accuracy: 0.0099\n",
      "Epoch 8/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 1.0948 - accuracy: 0.8614 - val_loss: 9.2529 - val_accuracy: 0.0099\n",
      "Epoch 9/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.5830 - accuracy: 0.8317 - val_loss: 10.5050 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.6716 - accuracy: 0.8911 - val_loss: 10.2686 - val_accuracy: 0.0099\n",
      "Epoch 11/50\n",
      "51/51 [==============================] - 3s 61ms/step - loss: 0.2687 - accuracy: 0.9307 - val_loss: 13.7289 - val_accuracy: 0.0099\n",
      "Epoch 12/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.4441 - accuracy: 0.9109 - val_loss: 11.2523 - val_accuracy: 0.0099\n",
      "Epoch 13/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.4501 - accuracy: 0.9010 - val_loss: 14.6522 - val_accuracy: 0.0198\n",
      "Epoch 14/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.4219 - accuracy: 0.8911 - val_loss: 14.9457 - val_accuracy: 0.0198\n",
      "Epoch 15/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.2769 - accuracy: 0.9505 - val_loss: 24.2732 - val_accuracy: 0.0297\n",
      "Epoch 16/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 1.5361 - accuracy: 0.7129 - val_loss: 10.2939 - val_accuracy: 0.0099\n",
      "Epoch 17/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.7839 - accuracy: 0.7822 - val_loss: 22.3593 - val_accuracy: 0.0099\n",
      "Epoch 18/50\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.8419 - accuracy: 0.8713 - val_loss: 12.7796 - val_accuracy: 0.0099\n",
      "Epoch 19/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.6081 - accuracy: 0.9208 - val_loss: 14.7833 - val_accuracy: 0.0297\n",
      "Epoch 20/50\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.6968 - accuracy: 0.8515 - val_loss: 15.9973 - val_accuracy: 0.0099\n",
      "Epoch 21/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 1.1199 - accuracy: 0.7822 - val_loss: 14.2562 - val_accuracy: 0.0198\n",
      "Epoch 22/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.5016 - accuracy: 0.8614 - val_loss: 20.5514 - val_accuracy: 0.0099\n",
      "Epoch 23/50\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.4108 - accuracy: 0.8812 - val_loss: 20.9071 - val_accuracy: 0.0198\n",
      "Epoch 24/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.2095 - accuracy: 0.9307 - val_loss: 23.7093 - val_accuracy: 0.0198\n",
      "Epoch 25/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.2934 - accuracy: 0.9307 - val_loss: 26.3404 - val_accuracy: 0.0198\n",
      "Epoch 26/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.2357 - accuracy: 0.9703 - val_loss: 27.3071 - val_accuracy: 0.0297\n",
      "Epoch 27/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.1649 - accuracy: 0.9505 - val_loss: 30.3472 - val_accuracy: 0.0198\n",
      "Epoch 28/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.3142 - accuracy: 0.9505 - val_loss: 26.3658 - val_accuracy: 0.0198\n",
      "Epoch 29/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.1199 - accuracy: 0.9604 - val_loss: 29.6202 - val_accuracy: 0.0099\n",
      "Epoch 30/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0442 - accuracy: 0.9901 - val_loss: 38.0488 - val_accuracy: 0.0198\n",
      "Epoch 31/50\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.0361 - accuracy: 0.9901 - val_loss: 35.9492 - val_accuracy: 0.0198\n",
      "Epoch 32/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 38.0612 - val_accuracy: 0.0198\n",
      "Epoch 33/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.3636 - accuracy: 0.9505 - val_loss: 27.4967 - val_accuracy: 0.0198\n",
      "Epoch 34/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0940 - accuracy: 0.9703 - val_loss: 30.8521 - val_accuracy: 0.0099\n",
      "Epoch 35/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0779 - accuracy: 0.9703 - val_loss: 27.1427 - val_accuracy: 0.0099\n",
      "Epoch 36/50\n",
      "51/51 [==============================] - 3s 56ms/step - loss: 0.1335 - accuracy: 0.9505 - val_loss: 31.8214 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.1366 - accuracy: 0.9604 - val_loss: 25.1770 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0334 - accuracy: 0.9901 - val_loss: 26.3355 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.1410 - accuracy: 0.9505 - val_loss: 25.7801 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.0480 - accuracy: 0.9802 - val_loss: 36.2804 - val_accuracy: 0.0099\n",
      "Epoch 41/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0230 - accuracy: 0.9901 - val_loss: 37.4934 - val_accuracy: 0.0099\n",
      "Epoch 42/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 39.3504 - val_accuracy: 0.0099\n",
      "Epoch 43/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 40.3421 - val_accuracy: 0.0099\n",
      "Epoch 44/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 40.0026 - val_accuracy: 0.0099\n",
      "Epoch 45/50\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 37.4942 - val_accuracy: 0.0099\n",
      "Epoch 46/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 37.6078 - val_accuracy: 0.0099\n",
      "Epoch 47/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.2888 - accuracy: 0.9901 - val_loss: 33.5019 - val_accuracy: 0.0099\n",
      "Epoch 48/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.5089 - accuracy: 0.9703 - val_loss: 22.9068 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "51/51 [==============================] - 3s 58ms/step - loss: 0.1010 - accuracy: 0.9703 - val_loss: 30.2597 - val_accuracy: 0.0099\n",
      "Epoch 50/50\n",
      "51/51 [==============================] - 3s 57ms/step - loss: 0.7840 - accuracy: 0.9109 - val_loss: 31.5316 - val_accuracy: 0.0198\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5cfc174580>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, epochs=50,validation_data=val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 1s 19ms/step - loss: 27.6904 - accuracy: 0.0099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[27.690393447875977, 0.009900989942252636]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(test_ds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
