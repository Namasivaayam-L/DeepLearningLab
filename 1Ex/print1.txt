1.	Calculate the following using Tensors.
          -	Matrix Multiplication
          -	Inverse of a matrix	



Matrix Multiplication:


a1=torch.tensor([[5,9],[2,2]],dtype=torch.float32)
a2=torch.tensor([[6,7],[9,10]],dtype=torch.float32)
C=torch.zeros((2,2),dtype=torch.float32)
for i in range(2):
    v=0
    for j in range(2):
        v=torch.dot(a1[i,:],a2[:,j])
        print("a:**",a1[i,:])
        print("b:**",a2[:,j])
        C[i][j]=v
print(C)
print(torch.matmul(a1,a2))
print(a1.mul(a2))
print(torch.inverse(C))
C=C.numpy()
print(C)

Output:

a:** tensor([5., 9.])
b:** tensor([6., 9.])
a:** tensor([5., 9.])
b:** tensor([ 7., 10.])
a:** tensor([2., 2.])
b:** tensor([6., 9.])
a:** tensor([2., 2.])
b:** tensor([ 7., 10.])
tensor([[111., 125.],
        [ 30.,  34.]])
tensor([[ 1.4167, -5.2083],
        [-1.2500,  4.6250]])


Inverse Of a Matrix:

N=2
inv = [None for _ in range(N)]
def getCofactor(A, temp, p, q, n):
    i = 0
    j = 0 
    for row in range(n):
 
        for col in range(n):
            if (row != p and col != q):
 
                temp[i][j] = A[row][col]
                j += 1
                if (j == n - 1):
                    j = 0
                    i += 1
def determinant(A, n):
    D = 0
    if (n == 1):
        return A[0][0]
    temp = [] 
    for i in range(N):
        temp.append([None for _ in range(N)])
    sign = 1  
    for f in range(n):
        getCofactor(A, temp, 0, f, n)
        D += sign * A[0][f] * determinant(temp, n - 1)
        sign = -sign
 
    return D
def adjoint(A, adj):
    if (N == 1):
        adj[0][0] = 1
        return
    sign = 1
    temp = []
    for i in range(N):
        temp.append([None for _ in range(N)])
    for i in range(N):
        for j in range(N):
            # Get cofactor of A[i][j]
            getCofactor(A, temp, i, j, N)
            sign = [1, -1][(i + j) % 2]
            adj[j][i] = (sign)*(determinant(temp, N-1))
def inverse(A, inverse):
    det = determinant(A, N)
    if (det == 0):
        print("Singular matrix, can't find its inverse")
        return False
    adj = []
    for i in range(N):
        adj.append([None for _ in range(N)])
    adjoint(A, adj)
    for i in range(N):
        for j in range(N):
            inverse[i][j] = adj[i][j] / det
    return True
def displays(A):
    for i in range(N):
        for j in range(N):
            print(round(A[i][j], 6), end=" ")
        print()
for i in range(N):
    inv[i] = [None for _ in range(N)]
A=[[3,2],[1,1]]
inverse(A,inv)
print(inv)
displays(inv)

Output:

[[1.0, -2.0], [-1.0, 3.0]]
1.0 -2.0 
-1.0 3.0 


2.	Demonstrate the Autograd functionality with examples.


import torch
from torch.autograd import Variable
from torch.autograd import grad
a = torch.tensor([2., 3.], requires_grad=True)
b = torch.tensor([6., 4.], requires_grad=True)
Q = 3*a**3 - b**2
external_grad = torch.tensor([1., 1.])
Q.backward(gradient=external_grad)
print(a.grad)
print(b.grad)
print(9*a**2 == a.grad)
print(-2*b == b.grad)

Output:

tensor([36., 81.])
tensor([-12.,  -8.])
tensor([True, True])
tensor([True, True])

3.	Implement AND function in Perceptron with Bipolar inputs and targets.



import torch
import numpy as np

def perceptron(inputs, weights):
    weighted_sum = torch.dot(inputs, weights)
    prediction = torch.where(weighted_sum > 0, torch.tensor(1.0), torch.tensor(-1.0))
    return prediction

def train_perceptron(inputs, targets, weights, learning_rate, epochs):
    for epoch in range(epochs):
        for i, input_ in enumerate(inputs):
            prediction = perceptron(input_, weights)
            error = targets[i] - prediction
            weights += learning_rate * error * input_
    return weights
def calculate_accuracy(inputs, targets, weights):
    correct = 0
    total = 0
    with torch.no_grad():
        for i, input_ in enumerate(inputs):
            prediction = perceptron(input_, weights)
            if prediction == targets[i]:
                correct += 1
            total += 1
    return correct / total

inputs = torch.tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]],dtype=torch.float32)
targets = torch.tensor([-1, -1, -1, 1],dtype=torch.float32)
weights = torch.zeros(2)
learning_rate = 0.1
epochs = 20

trained_weights = train_perceptron(inputs, targets, weights, learning_rate, epochs)
print("Trained Weights: ", trained_weights)
accuracy = calculate_accuracy(inputs, targets, trained_weights)
print("Accuracy: ", accuracy)

Output:

Trained Weights:  tensor([0.2000, 0.2000])
Accuracy:  1.0


4.	Implement Perceptron for numeric datasets
i)	EDA of the dataset should be displayed
ii)	Missing values should be handled
iii)	Plot the testing accuracy vs training accuracy



import torch
from sklearn import datasets
from sklearn.model_selection import train_test_split
import pandas as pd
df=pd.read_csv("C:\\Users\\ponku\\OneDrive\\Desktop\\Desktop\\Study\\sem-6\\Deep Learning\\Lab\\ex-1\\Dataset_Smoking\\train_dataset.csv",encoding="UTF-8")
x=df.iloc[:,:-1]
y=df.iloc[:,-1]
print(x)
print(y)
X_train, X_test, Y_train, Y_test = train_test_split(x, y, train_size=0.8, stratify=y, random_state=123)
X_train = torch.tensor(X_train.values,dtype=torch.float32)
X_test=torch.tensor(X_test.values,dtype=torch.float32)
Y_train=torch.tensor(Y_train.values, dtype=torch.long)
Y_test=torch.tensor(Y_test.values, dtype=torch.long)
samples, features = X_train.shape
classes = Y_test.unique()
print(features)
print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)
from torch import nn
class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.first_layer = nn.Linear(features, 5)
        self.second_layer = nn.Linear(5, 10)
        self.third_layer = nn.Linear(10, 15)
        self.fourth_layer = nn.Linear(15, 20)
        self.fifth_layer = nn.Linear(20, 25)
        self.sixth_layer = nn.Linear(25, 30)
        self.final_layer = nn.Linear(30,2)
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

    def forward(self, X_batch):
        layer_out = self.relu(self.first_layer(X_batch))
        layer_out = self.relu(self.second_layer(layer_out))
        layer_out = self.relu(self.third_layer(layer_out))
        layer_out = self.relu(self.fourth_layer(layer_out))
        layer_out = self.relu(self.fifth_layer(layer_out))
        layer_out = self.relu(self.sixth_layer(layer_out))
        return self.sigmoid(self.final_layer(layer_out))
classifier = Classifier()
preds = classifier(X_train[:5])
preds
def TrainModel(model, loss_func, optimizer, X, Y, epochs=500):
    for i in range(epochs):
        preds = model(X) 
        loss = loss_func(preds, Y) 
        optimizer.zero_grad() 
        optimizer.step()
        if i % 100 == 0:
            print("NegLogLoss : {:.2f}".format(loss))
from torch.optim import SGD
torch.manual_seed(42) ##For reproducibility.This will make sure that same random weights are initialized each time.
epochs = 2000
learning_rate = torch.tensor(1/1e3) # 0.01
classifier = Classifier()
nll_loss = nn.NLLLoss()
optimizer = SGD(params=classifier.parameters(), lr=learning_rate)
TrainModel(classifier, nll_loss, optimizer, X_train, Y_train, epochs=epochs)
test_preds = classifier(X_test) 
test_preds = torch.argmax(test_preds, axis=1) 
train_preds = classifier(X_train)
train_preds = torch.argmax(train_preds, axis=1)
test_preds[:5], train_preds[:5]
from sklearn.metrics import accuracy_score
print("Train Accuracy : {:.2f}".format(accuracy_score(Y_train, train_preds)))
print("Test  Accuracy : {:.2f}".format(accuracy_score(Y_test, test_preds)))
from sklearn.metrics import classification_report
print("Test Data Classification Report : ")
print(classification_report(Y_test, test_preds))


Output:

NegLogLoss : -0.53
NegLogLoss : -0.53
NegLogLoss : -0.53
NegLogLoss : -0.53
NegLogLoss : -0.53
NegLogLoss : -0.53
Train Accuracy : 0.63
Test  Accuracy : 0.63
Test Data Classification Report : 
              precision    recall  f1-score   support

           0       0.63      1.00      0.78      4933
           1       0.00      0.00      0.00      2864

    accuracy                           0.63      7797
   macro avg       0.32      0.50      0.39      7797
weighted avg       0.40      0.63      0.49      7797

