{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 07:00:24.566490: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 07:00:25.425753: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2023-04-24 07:00:25.425884: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2023-04-24 07:00:25.425894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-04-24 07:00:26.706382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 07:00:26.713731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 07:00:26.713788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 07:00:26.714262: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-24 07:00:26.719418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 07:00:26.719486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 07:00:26.719512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 07:00:27.467031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 07:00:27.467162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 07:00:27.467174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-04-24 07:00:27.467216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-04-24 07:00:27.467252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-04-24 07:00:28.660974: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x562f616d0ee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-24 07:00:28.661078: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce GTX 1650, Compute Capability 7.5\n",
      "2023-04-24 07:00:28.705966: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-24 07:00:29.133259: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-04-24 07:00:29.212583: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.484467119\n",
      "Epoch: 0002 cost= 0.484474480\n",
      "Epoch: 0003 cost= 0.484556794\n",
      "Epoch: 0004 cost= 0.484459102\n",
      "Epoch: 0005 cost= 0.484423399\n",
      "Epoch: 0006 cost= 0.484411508\n",
      "Epoch: 0007 cost= 0.484471679\n",
      "Epoch: 0008 cost= 0.484502405\n",
      "Epoch: 0009 cost= 0.484418690\n",
      "Epoch: 0010 cost= 0.484424293\n",
      "Validation loss: tf.Tensor(0.4848126, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Define the RBM model\n",
    "class RBM(object):\n",
    "    def __init__(self, n_visible, n_hidden):\n",
    "        self.W = tf.Variable(tf.random.normal([n_visible, n_hidden], 0.01))\n",
    "        self.hb = tf.Variable(tf.zeros([1, n_hidden], tf.float32))\n",
    "        self.vb = tf.Variable(tf.zeros([1, n_visible], tf.float32))\n",
    "        \n",
    "    def sample_hidden(self, x):\n",
    "        h_prob = tf.nn.sigmoid(tf.matmul(x, self.W) + self.hb)\n",
    "        return h_prob, tf.nn.relu(tf.sign(h_prob - tf.random.uniform(tf.shape(h_prob))))\n",
    "    \n",
    "    def sample_visible(self, y):\n",
    "        v_prob = tf.nn.sigmoid(tf.matmul(y, tf.transpose(self.W)) + self.vb)\n",
    "        return v_prob, tf.nn.relu(tf.sign(v_prob - tf.random.uniform(tf.shape(v_prob))))\n",
    "\n",
    "# Set hyperparameters\n",
    "n_visible = 784\n",
    "n_hidden = 256\n",
    "lr = 0.1\n",
    "n_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], n_visible)\n",
    "train_data = train_images.astype(np.float32) / 255.\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "val_data = train_data[-10000:]\n",
    "train_data = train_data[:-10000]\n",
    "\n",
    "# Create RBM model\n",
    "rbm = RBM(n_visible, n_hidden)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = tf.optimizers.Adam(lr)\n",
    "loss_fn = tf.losses.MeanSquaredError()\n",
    "\n",
    "# Train RBM model on training dataset\n",
    "for epoch in range(n_epochs):\n",
    "    avg_cost = 0.0\n",
    "    for i in range(0, train_data.shape[0], batch_size):\n",
    "        batch = train_data[i:i+batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            hidden_prob, hidden_state = rbm.sample_hidden(batch)\n",
    "            visible_prob, visible_state = rbm.sample_visible(hidden_state)\n",
    "            cost = loss_fn(batch, visible_state)\n",
    "        grads = tape.gradient(cost, [rbm.W, rbm.hb, rbm.vb])\n",
    "        optimizer.apply_gradients(zip(grads, [rbm.W, rbm.hb, rbm.vb]))\n",
    "        avg_cost += cost / (train_data.shape[0] / batch_size)\n",
    "    print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    \n",
    "# Evaluate RBM model on validation dataset\n",
    "hidden_prob, hidden_state = rbm.sample_hidden(val_data)\n",
    "visible_prob, visible_state = rbm.sample_visible(hidden_state)\n",
    "val_loss = loss_fn(val_data, visible_state)\n",
    "print(\"Validation loss:\", val_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
